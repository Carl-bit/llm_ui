# llm_ui
Genere una WebUI para compartir de mejor manera que ollama se esta ejecutando en mi servidor 
